# -*- coding: utf-8 -*-
"""AdEase_Time_Series_by_Pritam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O1bBQzBzle8LGeqwyXal4R8Bobdn_ZIy
"""

# Importing the required libraries
# Pandas and Numpy libraries will be used for data manipulations, Matplotlib and Seaborn will be used for data visualizations which will be required for Analysis.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

df = pd.read_csv('/content/train_1.csv')
campaign_data = pd.read_csv('/content/Exog_Campaign_eng')

df.head()

df.shape

campaign_data.head()

campaign_data.shape

df['Page'].sample(20)     #Let's just look at some random Page values to understand the page name format to extract some useful values

df['Page'].str.extract(r'[^_]+_[^_]+_[^_]+_([^_]+)').head(10)

data = df.copy()

data.duplicated().sum()   # There seems to be no duplicate data

data.info() # let's look at the data type

indexes = data.head(2).columns[1:][range(0,549,20)].values         # Let's reduce the number of dates or compress the indexes
indexes

# Check for missing values
plt.figure(figsize=(15, 5))
data.isna().sum()[indexes].plot(linestyle='dotted').set_title('Missing Values')
plt.show()

"""From above plot , we can observe that with time , null values are decreasing.
Recent dates have lesser null values, which means newer pages will have no data of prior to that page hosting date.
"""

data.fillna(0,inplace =True)   # We'll replace the null values with 0

data[indexes].isna().sum()    # No missing values are observed for our indexes

"""### **Extracting Language**"""

data.loc[0, "Page"]

import re

re.findall(r'_(.{2}).wikipedia.org_', "2NE1_zh.wikipedia.org_all-access_spider")

data["Page"].str.findall(pat="_(.{2}).wikipedia.org_").sample(10)

def extract_language(name):
  if len(re.findall(r'_(.{2}).wikipedia.org_', name)) == 1 :
    return re.findall(r'_(.{2}).wikipedia.org_', name)[0]
  else:
    return 'Unknown'

data["Language"] = data["Page"].map(extract_language)

data["Language"].unique()

dict_ ={'de':'German',
 'en':'English',
 'es': 'Spanish',
 'fr': 'French',
 'ja': 'Japenese' ,
 'ru': 'Russian',
 'zh': 'Chinese',
 'Unknown': 'Unknown_Language'}

data["Language"] = data["Language"].map(dict_)            # Let's create a dictionary of all the abbreviations along with the actual language names for all the visible languages

data.head()

plt.figure(figsize=(8, 4))
data.groupby("Language")["Page"].count().plot(kind="bar")
plt.xlabel("Language")
plt.ylabel("Number of Pages")
plt.title("Number of Pages per each language")
plt.show()

from locale import normalize

data["Language"].value_counts(normalize=True) * 100               # Let's look at the proportions of each language data that we have

"""16.61 % of all pages are in English which is the highest.

12.30 % of pages have unknown language.

### **Extracting Access Type**
"""

data["Access_Type"] = data["Page"].str.findall(r'all-access|mobile-web|desktop').apply(lambda x: x[0] if len(x) == 1 else 'Unknown')

data["Access_Type"].value_counts(dropna=False, normalize=True)

x = (data["Access_Type"].value_counts(dropna=False, normalize=True) * 100).values
y = (data["Access_Type"].value_counts(dropna=False, normalize=True) * 100).index

plt.figure(figsize=(8, 6))
plt.pie(x, labels=y, autopct='%1.1f%%')
plt.title('Access Type Distribution', fontweight='bold')
plt.show()

"""### **Extracting Access Origin**"""

data["Page"].sample(10)

data.Page.str.findall(r'spider|agents').apply(lambda x:x[0]).isna().sum()

data["Access_Origin"] = data.Page.str.findall(r'spider|agents').apply(lambda x:x[0])

data["Access_Origin"].value_counts(dropna= False, normalize=True) * 100

x = (data["Access_Origin"].value_counts(dropna= False, normalize=True) * 100).values
y = (data["Access_Origin"].value_counts(dropna= False, normalize=True) * 100).index

plt.figure(figsize=(8, 6))
plt.pie(x, labels=y, autopct='%1.1f%%')
plt.title('Access Origin Distribution', fontsize = 15, fontweight = 'bold')
plt.show()

data.head()

indexes

numeric_columns = data.select_dtypes(include=np.number).columns.tolist()

# Perform the groupby and mean calculation on numeric columns
data.groupby("Language")[numeric_columns].mean()

pd.set_option('display.max_rows', 500)

aggregated_data = data.groupby("Language")[numeric_columns].mean().T.drop("Unknown_Language",axis = 1).reset_index()
aggregated_data["index"] = pd.to_datetime(aggregated_data["index"])
aggregated_data = aggregated_data.set_index("index")
aggregated_data

aggregated_data.info()

aggregated_data.index

"""### **Visualising Time Series for each languages**"""

plt.rcParams['figure.figsize'] = (15, 12)
aggregated_data.plot()
plt.xlabel("Time Index")
plt.ylabel("Visits Per Each Language")
plt.show()

"""As per plot, english language is most visited language w.r.t time index. In month of July to
September it is in at its peak.

### **Hypothesis Testing : If Time Series is Stationary or Trending**



*   Null Hypothesis: The series is Non-Stationary
*   Alternative Hypothesis: The series is Stationary

significance value : 0.05 (alpha)

if p-value > 0.05 : we failed to reject Null hypothesis:
That means the series is Non-Stationary.

if p-value <= 0.05: we reject Null Hypothesis
that means the time series in Stationary
"""

import statsmodels.api as sm

def Dickey_Fuller_test(ts,significance_level = 0.05):
 p_value = sm.tsa.stattools.adfuller(ts)[1]

 if p_value <= significance_level:
  print("Time Series is Stationary")
  print("P_value is: ", p_value)
 else:
  print("Time Series is Not Stationary")
  print("P_value is: ", p_value)

for Language in aggregated_data.columns:
 print(Language)
 print(Dickey_Fuller_test(aggregated_data[Language],significance_level = 0.05))
 print("\n")

"""Based on DickeyFuller test of Stationarity , we can observe Spanish and Russian languages Pages visits Time series are stationary.

Chinese, English , German , Japanese and French are not stationary.

### **Analysing Time Series for English Language Pages Visits**
"""

aggregated_data

TS_English = aggregated_data["English"]

def adf_test(timeseries):
 print ('Results of Dickey-Fuller Test:')

 dftest = sm.tsa.stattools.adfuller(timeseries, autolag='AIC')
 df_output = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used', 'Number of Observations Used', 'Critical Value (1%)', 'Critical Value (5%)', 'Critical Value (10%)'])
 for key, value in dftest[4].items():
   df_output['Critical Value (%s)' %key] = value
 print (df_output)

Dickey_Fuller_test(TS_English)

"""### **Visualising English-Language Page Visits Time Series manually to identify seasonality and period**"""

plt.rcParams['figure.figsize'] = (15, 3)

TS_English[:8].plot()
plt.show()

TS_English[8:15].plot()
plt.show()

TS_English[15:22].plot()
plt.show()

TS_English[22:29].plot()
plt.show()

TS_English[29:36].plot()
plt.show()

correlations = []
for lag in range(1,30):
  present = TS_English[:-lag]
  past = TS_English.shift(-lag)[:-lag]
  corrs = np.corrcoef(present,past)[0][-1]
  print(lag,corrs)
  correlations.append(corrs)

"""### **Time Series Decomposition**

Y(t) = seasonality S(t) + trend T(t) + residuals R(t)

"""

# using auto correlation function plot , to varify the period
from statsmodels.graphics.tsaplots import plot_acf
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (12, 6)
plot_acf(TS_English, lags=56, color='green');

plt.rcParams['figure.figsize'] = (15, 10)

Decomposition_model = sm.tsa.seasonal_decompose(TS_English, model='additive', period=30)
Decomposition_model.plot();

Dickey_Fuller_test(pd.Series(Decomposition_model.resid).fillna(0))

# Taking the first differentiation of the time series and plotting

plt.rcParams['figure.figsize'] = (15, 3)
TS_English.diff(1).dropna().plot()

Dickey_Fuller_test(TS_English.diff(1).dropna())

"""After 1 differentiation, time series becomes stationary.

Thus for ARIMA models , we can set d = 1

"""

from sklearn.metrics import (mean_squared_error as mse,  mean_absolute_error as mae, mean_absolute_percentage_error as mape)

# Creating a function to print values of all these metrics.
def performance(actual, predicted):
 print('MAE :', round(mae(actual, predicted), 3))
 print('RMSE :', round(mse(actual, predicted)**0.5, 3))
 print('MAPE:', round(mape(actual, predicted), 3))

"""### **Forecasting**"""

TS_English.index.freq = 'D'
model = sm.tsa.ExponentialSmoothing(TS_English, seasonal='add',trend="add")
model = model.fit()

TS_English.tail(100).plot(style='-o', label='actual')
model.forecast(30).plot(style='-o', label='predicted')
plt.show()

X_train = TS_English.loc[TS_English.index < TS_English.index[-30] ].copy()
X_test = TS_English.loc[TS_English.index >= TS_English.index[-30] ].copy()

import warnings # supress warnings
warnings.filterwarnings('ignore')

model = sm.tsa.ExponentialSmoothing(X_train, trend="add", damped_trend="add", seasonal="add")
model = model.fit(smoothing_level=None, # alpha
                  smoothing_trend=None, # beta
                  smoothing_seasonal=None) # gama)

# X_test.plot()
Pred = model.forecast(steps=30)
performance(X_test,Pred)

X_test.plot(style="-o",label ="Test_data")
Pred.plot(label="Predicted_data")

plt.legend()
plt.show()

"""### **ARIMA**

*   Autoregressive Integrated Moving Average (ARIMA) model, and extensions
*   This model is the basic interface for ARIMA-type models, including those with exogenous regressors and those with seasonal components. The most general form of the model is SARIMAX(p, d, q)x(P, D, Q, s). It also allows all specialized cases, including autoregressive models: AR(p)


moving average models: MA(q)

mixed autoregressive moving average models: ARMA(p, q)

integration models: ARIMA(p, d, q)

seasonal models: SARIMA(P, D, Q, s)

regression with errors that follow one of the above ARIMA-type models
"""

from statsmodels.tsa.arima.model import ARIMA
TS = TS_English.copy(deep=True)

n_forecast = 30
model = ARIMA(TS[:-n_forecast],order = (1,1,1))
model = model.fit()
predicted = model.forecast(steps= n_forecast, alpha = 0.05)

TS.plot(label = 'Actual')
predicted.plot(label = 'Forecast', linestyle='dashed', marker='o', markerfacecolor='g')

plt.legend(loc="upper right")
plt.title('ARIMA BASE Model (1,1,1) : Actual vs Forecasts', fontsize = 15, fontweight = 'bold')
plt.show()

#Calculating MAPE & RMSE
actuals = TS.values[-n_forecast:]
errors = TS.values[-n_forecast:] - predicted.values

mape = np.mean(np.abs(errors)/ np.abs(actuals))
rmse = np.sqrt(np.mean(errors**2))

print("\n")
print(f'MAPE of Model : {np.round(mape,5)}')
print(f'RMSE of Model : {np.round(rmse,3)}')

"""### **SARIMAX model**"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.statespace.sarimax import SARIMAX

def sarimax_model(time_series, n, p=0, d=0, q=0, P=0, D=0, Q=0, s=0, exog = []):

 #Creating SARIMAX Model with order(p,d,q) & seasonal_order=(P, D, Q, s)
 model = SARIMAX(time_series[:-n], order =(p,d,q), seasonal_order=(P, D, Q, s), exog = exog[:-n], initialization='approximate_diffuse')
 model_fit = model.fit()

 #Creating forecast for last n-values
 model_forecast = model_fit.forecast(n, dynamic = True, exog = pd.DataFrame(exog[-n:]))

 #plotting Actual & Forecasted values
 plt.figure(figsize = (20,8))
 time_series[-60:].plot(label = 'Actual')
 model_forecast[-60:].plot(label = 'Forecast', color = 'red', linestyle='dashed', marker='o', markerfacecolor='green')
 plt.legend(loc="upper right")
 plt.title(f'SARIMAX Model ({p},{d},{q}) ({P},{D},{Q},{s}) : Actual vs Forecasts')
 plt.show()

 #Calculating MAPE & RMSE
 actuals = time_series.values[-n:]
 errors = time_series.values[-n:] - model_forecast.values
 mape = np.mean(np.abs(errors)/ np.abs(actuals))

 rmse = np.sqrt(np.mean(errors**2))

 print()
 print(f'MAPE of Model : {np.round(mape,5)}')
 print(f'RMSE of Model : {np.round(rmse,3)}')

exog = campaign_data['Exog'].to_numpy()
time_series = aggregated_data.English
test_size= 0.1
p,d,q, P,D,Q,s = 1,1,1,1,1,1,7
n = 30
sarimax_model(time_series, n, p=p, d=d, q=q, P=P, D=D, Q=Q, s=s, exog = exog)

"""### **Hyperparamer tuning for SARIMAX model**"""

def SARIMAX_grid_search(time_series, n, param, d_param, s_param, exog=[]):
 counter = 0
 param_df = pd.DataFrame(columns=['serial', 'pdq', 'PDQs', 'mape', 'rmse'])
 for p in param:
    for d in d_param:
      for q in param:
        for P in param:
          for D in d_param:
            for Q in param:
              for s in s_param:
                try:
                  model = SARIMAX(time_series[:-n], order=(p, d, q), seasonal_order=(P, D, Q, s), exog=exog[:-n])
                  model_fit = model.fit()

                  model_forecast = model_fit.forecast(n, dynamic=True, exog=pd.DataFrame(exog[-n:]))

                  actuals = time_series.values[-n:]
                  errors = time_series.values[-n:] - model_forecast.values

                  mape = np.mean(np.abs(errors) / np.abs(actuals))
                  rmse = np.sqrt(np.mean(errors ** 2))
                  mape = np.round(mape, 5)
                  rmse = np.round(rmse, 3)

                  counter += 1
                  list_row = [counter, (p, d, q), (P, D, Q, s), mape, rmse]
                  param_df.loc[len(param_df)] = list_row
                  print(f'Possible Combination: {counter} out of {len(param) * len(d_param) * len(param) * len(param) * len(d_param) * len(param) * len(s_param)}')
                except:
                  continue

 return param_df

#Finding best parameters for English time series
exog = campaign_data['Exog'].to_numpy()
time_series = aggregated_data.English
n = 30
param = [0,1,2]
d_param = [0,1]
s_param = [1]
english_params = SARIMAX_grid_search(time_series, n, param, d_param,s_param, exog)

english_params.sort_values(['mape', 'rmse']).head()

exog = campaign_data['Exog'].to_numpy()
time_series = aggregated_data.English
test_size= 0.1
p,d,q, P,D,Q,s = 2,1,2,1,1,2,7
n = 30
sarimax_model(time_series, n, p=p, d=d, q=q, P=P, D=D, Q=Q, s=s, exog = exog)

"""### **Hyperparameter tuning for all other languages**"""

def pipeline_sarimax_grid_search_without_exog(languages, data, n, param, d_param, s_param):

  best_param_df = pd.DataFrame(columns = ['language','p','d', 'q', 'P','D','Q','s', 'mape'])
  for lang in languages:
      print('')
      print('')
      print(f'--------------------------------------------------------------')
      print(f' Finding best parameters for {lang} ')
      print(f'--------------------------------------------------------------')
      counter = 0
      time_series = data[lang]
      best_mape = 100

      #Creating loop for every paramater to fit SARIMAX model
      for p in param:
        for d in d_param:
          for q in param:
            for P in param:
              for D in d_param:
                for Q in param:
                  for s in s_param:
                    #Creating Model
                    model = SARIMAX(time_series[:-n], order=(p,d,q), seasonal_order=(P, D, Q, s), initialization='approximate_diffuse')
                    model_fit = model.fit()

                    #Creating forecast from Model
                    model_forecast = model_fit.forecast(n, dynamic = True)

                    #Calculating errors for results
                    actuals = time_series.values[-n:]
                    errors = time_series.values[-n:] - model_forecast.values

                    #Calculating MAPE & RMSE
                    mape = np.mean(np.abs(errors)/ np.abs(actuals))

                    counter += 1

                    if (mape < best_mape):
                      best_mape = mape
                      best_p = p
                      best_d = d
                      best_q = q
                      best_P = P
                      best_D = D
                      best_Q = Q
                      best_s = s
                    else: pass


            #print statement to check progress of Loop
            print(f'Possible Combination: {counter} out of {(len(param)**4) * (len(d_param)**2) * (len(param)**4) * (len(d_param)**2) * (len(param)**4) * (len(s_param))}')

      best_mape = np.round(best_mape, 5)
      print(f'--------------------------------------------------------------')
      print(f'Minimum MAPE for {lang} = {best_mape}')
      print(f'Corresponding Best Parameters are {best_p , best_d, best_q, best_P, best_D, best_Q, best_s}')
      print(f'--------------------------------------------------------------')

      best_param_row = [lang, best_p, best_d, best_q, best_P, best_D, best_Q, best_s, best_mape]
      best_param_df.loc[len(best_param_df)] = best_param_row

  return best_param_df

languages = aggregated_data.columns
n = 30
param = [0,1,0]
d_param = [0,1]
s_param = [2]
best_param_df = pipeline_sarimax_grid_search_without_exog(languages, aggregated_data, n, param, d_param, s_param)

best_param_df

best_param_df.sort_values(['mape'], inplace = True)
best_param_df

def plot_best_SARIMAX_model(languages, data, n, best_param_df):

 for lang in languages:
    #fetching respective best parameters for that language
    p = best_param_df.loc[best_param_df['language'] == lang, ['p']].values[0][0]
    d = best_param_df.loc[best_param_df['language'] == lang, ['d']].values[0][0]
    q = best_param_df.loc[best_param_df['language'] == lang, ['q']].values[0][0]
    P = best_param_df.loc[best_param_df['language'] == lang, ['P']].values[0][0]
    D = best_param_df.loc[best_param_df['language'] == lang, ['D']].values[0][0]
    Q = best_param_df.loc[best_param_df['language'] == lang, ['Q']].values[0][0]
    s = best_param_df.loc[best_param_df['language'] == lang, ['s']].values[0][0]

    #Creating language time-series
    time_series = data[lang]

    #Creating SARIMAX Model with order(p,d,q) & seasonal_order=(P, D, Q, s)
    model = SARIMAX(time_series[:-n],
    order =(p,d,q),
    seasonal_order=(P, D, Q, s),
    initialization='approximate_diffuse')
    model_fit = model.fit()

    #Creating forecast for last n-values
    model_forecast = model_fit.forecast(n, dynamic = True)

    #Calculating MAPE & RMSE
    actuals = time_series.values[-n:]
    errors = time_series.values[-n:] - model_forecast.values
    mape = np.mean(np.abs(errors)/ np.abs(actuals))
    rmse = np.sqrt(np.mean(errors**2))

    print('')
    print('')
    print(f'--------------------------------------------------------------------')
    print(f' SARIMAX model for {lang} Time Series')
    print(f' Parameters of Model : ({p},{d},{q}) ({P},{D},{Q},{s})')
    print(f' MAPE of Model : {np.round(mape,5)}')
    print(f' RMSE of Model : {np.round(rmse,3)}')
    print(f'--------------------------------------------------------------------')

    #plotting Actual & Forecasted values
    time_series.index = time_series.index.astype('datetime64[ns]')
    model_forecast.index = model_forecast.index.astype('datetime64[ns]')
    plt.figure(figsize = (20,8))
    time_series[-60:].plot(label = 'Actual')
    model_forecast[-60:].plot(label = 'Forecast', color = 'red', linestyle='dashed', marker='o', markerfacecolor='green')

    plt.legend(loc="upper right")
    plt.title(f'SARIMAX Model ({p},{d},{q}) ({P},{D},{Q},{s}) : Actual vs Forecasts')
    plt.show()

 return 0

#Plotting SARIMAX model for each Language Time Series
languages = aggregated_data.columns
n = 30
plot_best_SARIMAX_model(languages, aggregated_data, n, best_param_df)

"""### **Forecasting using Facebook Prophet**"""

!pip install prophet

from prophet import Prophet

time_series = aggregated_data.reset_index()
time_series = time_series[['index', 'English']]
time_series.columns = ['ds', 'y']
time_series['ds'] = pd.to_datetime(time_series['ds'])   # Ensure 'ds' column is in dataframe

exog = campaign_data.copy(deep=True)
time_series['exog'] = exog.values

time_series

"""## **Insights**

*   There were 7 languages in all in the data.
*   The language with the most pages is English.
*   Three categories of access:
      1. 51.22 % of All-access
      2. Mobile web 24.77 percent
      3. 23.99 % on a desktop
*   Two points of access:
      1. The language with the most pages is English (74.93%); Agents (74.93%); Spider (24.06%).
      2. The English page ought to run the most advertising possible.

## **Recommendations**

*   To ascertain whether the time series is stationary, do an enhanced Dickey-Fuller test.
*   Fit an ARMA model if the time series is stationary. If it is non-stationary, find out what d is.
*   Plot the data's autocorrelation and partial autocorrelation graphs once stationarity has been attained.
*   Since the cut-off point in the partial autocorrelation graph (PACF) equals p, plot the PACF to find the value of p.
*   Plot the autocorrelation graph (ACF) to find q, since q is the value of the ACF's cut-off point.
"""

